# -*- coding: utf-8 -*-
"""FaceR_v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HSQT8CkZQMOblZz-j79YvCFdnmyPQyqn
"""

#!pip install kafka-python
#!pip install mtcnn

import base64
from matplotlib import pyplot as plt
from mtcnn.mtcnn import MTCNN
from numpy import asarray
from PIL import Image
import io
import json
from kafka import KafkaProducer
from kafka import KafkaConsumer

def base64_encode_image(a):
	# base64 encode the input NumPy array
	return base64.b64encode(a).decode("utf-8")
def base64_decode_image(a, dtype, shape):
	# if this is Python 3, we need the extra step of encoding the
	# serialized NumPy string as a byte object
	if sys.version_info.major == 3:
		a = bytes(a, encoding="utf-8")
	# convert the string to a NumPy array using the supplied data
	# type and target shape
	a = np.frombuffer(base64.decodebytes(a), dtype=dtype)
	a = a.reshape(shape)
	# return the decoded image
	return a

#with open("person.jpg", "rb") as image_file:
#    encoded_string = base64.b64encode(image_file.read())
import cv2
import codecs
from google.colab.patches import cv2_imshow
import numpy as np
detector = MTCNN()
tmp_face_dict = {}
def msg_process (msg,producer):
  id=msg["id"]
  base64image=msg["image"]

  image = base64.b64decode(base64image)
  jpg_as_np = np.frombuffer(image, dtype=np.uint8)
  image=cv2.imdecode(jpg_as_np, flags=1)

  print('Original Dimensions : ',image.shape)
 
  '''
  fixed_height = 420
  height_percent = (fixed_height / float(image.size[1]))
  width_size = int((float(image.size[0]) * float(height_percent)))
  image_resized = image.resize((width_size, fixed_height), PIL.Image.NEAREST)
  '''
  scale_percent = 100 # percent of original size
  width = int(image.shape[1] * scale_percent / 100)
  height = int(image.shape[0] * scale_percent / 100)
  dim = (width, height)
  image_resized=cv2.resize(image, dim, interpolation = cv2.INTER_AREA)
  image_resized2=image_resized
  print('Resized Dimensions : ',image_resized.shape)

  faces = detector.detect_faces(image_resized)
  face_nr = 0;
  for face in faces:
      face_nr=face_nr+1
      face_dict={'id': id}
      face_dict['face_nr']=face_nr
      face_dict['object_box']=list(face.values())[0]

      x, y, w, h = face['box']
      
      x=x-20
      y=y-40
      w=w+40
      h=h+50
      
      img_cropped=image_resized[y:y+h, x:x+w]
      cv2_imshow(img_cropped)

      retval, buffer = cv2.imencode('.jpg', img_cropped)
      jpg_as_text = base64.b64encode(buffer)
      jpg_as_textv2=codecs.decode(jpg_as_text, 'UTF-8')
      face_dict['image_cut_out']=jpg_as_textv2
      print(face_dict)
      
      producer.send('faces_found_detail', face_dict, b"image")
      
      cv2.rectangle(image_resized2,(x,y),(x+w,y+h),(0,0,255),2)
      #now draw rectangle for every face on original image

  #after all rectangle are drawn encode the image back to base64
  #send the base64 image with all rectangles to GUI
  cv2_imshow(image_resized2)
  retval2, buffer2 = cv2.imencode('.jpg', image_resized2)
  jpg_as_text2 = base64.b64encode(buffer2)

  jpg_as_text2v2=codecs.decode(jpg_as_text2, 'UTF-8')

  image_face_dict={'id': id}
  image_face_dict['image_with_boxes']=jpg_as_text2v2
  #print(image_face_dict[:100])
  producer.send('faces_found', image_face_dict, b"image")
  print("sended")

def basic_consume_loop(consumer,producer):
    try:
        while running:
            for msg in consumer:
              if msg is None: continue

              #if msg.error():
              #    if msg.error().code() == KafkaError._PARTITION_EOF:
              #        # End of partition event
              #        sys.stderr.write('%% %s [%d] reached end at offset %d\n' %
              #                        (msg.topic(), msg.partition(), msg.offset()))
              #    elif msg.error():
              #        raise KafkaException(msg.error())
              #else:
              msg_process(msg,producer)
    finally:
        # Close down consumer to commit final offsets.
        consumer.close()

def shutdown():
    running = False

consumer = KafkaConsumer('object-detection', bootstrap_servers=['SERVER_NAME'], auto_offset_reset='latest', value_deserializer=lambda x: json.loads(x.decode('utf-8')))
producer = KafkaProducer(bootstrap_servers='SERVER_NAME',value_serializer=lambda v:json.dumps(v).encode('utf-8'))
running = True
while running:
  for msg in consumer:
    print(msg)
    print(msg.value)
    msg_process(msg.value,producer)
#basic_consume_loop(consumer,producer)



